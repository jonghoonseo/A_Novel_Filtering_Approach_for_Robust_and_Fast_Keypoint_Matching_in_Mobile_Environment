%!TEX root = ../A_Novel_Filtering_Approach_for_Robust_and_Fast_Keypoint_Matching_in_Mobile_Environment.tex

\section{Related Works}
In general, the framework of conventional keypoint-based matching systems consists of three major steps, including keypoint detection, description and matching. It is usually called as the detection-describe-match (DDM) framework \cite{yu_novel_2012}. In this section, we will summarize the current state-of-the-art techniques in each procedure with their pros and cons. Moreover, conventional keypoint filtering methods related to our work will be discussed.

%=====================================================================================================
\subsection{Keypoint Detection}
The first step of the keypoint-based matching is the keypoint detection. Generally, most of researches use corners(Harris\cite{harris_combined_1988}, SUSAN\cite{smith_susannew_1997}, etc) or center of silent region(SIFT\cite{lowe_distinctive_2004}, SURF\cite{bay_speeded-up_2008}, etc) as the keypoints (or interesting points) since they are stable against various situations and easy to locate and describe\cite{yu_novel_2012}.

\subsubsection{Corner Detection}
Among several local features, corners are considered as the first low-level features used for image matching. Considering corners as intersection of two edges, corners have no spatial extension and, therefore, there is no ambiguity in their location. Moravec\cite{moravec_obstacle_1980} computes the sum-of-squared-differences between a patch around a candidate corner and patches shifted a small distance in a number of directions. Based on Moravec's work, Harris and Stephens\cite{harris_combined_1988} developed the Harris Corner Detector, probably one of the most popular corner detection methods. It is based on the first order Taylor expansion of the second derivative of the local SSD with respect to the shift. Mikolajczyk and Schmid\cite{mikolajczyk_indexing_2001} proposed an approach to make the Harris detector scale invariant. 
Based on the assumption of affine image deformation, Shi and Tomasi\cite{shi_good_1994} obtained the same equation by analyzing the optical flow equation proposed by Lucas and Kanade\cite{lucas_iterative_1981}. Other intensity-based corner detectors include the method of Beaudet\cite{beaudet_rotationally_1978}, which uses the determinant of the Hessian matrix, and the method from Kitchen and Rosenfeld\cite{kitchen_gray-level_1982}, which measures the change of direction in the local gradient field. 

To avoid computationally expensive window or filtering operations, several methods have been proposed to detect keypoints by examining a small patch of an image. Since second derivatives are not computed, a noise reduction step (such as Gaussian smoothing) is not required. Consequently, these corner detectors are computationally efficient since only a small number of pixels are examined for each corner detected. Smith and Brady proposed the keypoint detection method so called "Smallest Uni-value Segment Assimilating Nucleus (SUSAN)"\cite{smith_susannew_1997}. The brightness of the pixel to be tested, the nucleus, is compared to its circular neighborhood pixels, and the area of the uni-value segment assimilating nucleus(USAN) is computed. Corner and edges can be detected by evaluating this area, or it can also be used for noise reduction. Trajkovic and Hedley\cite{trajkovic_fast_1998} used a similar idea to find corner points: the pixel value at the center of a discretized circle is compared to the values of pixels on the circle. Based on this idea, Rosten and Drummond proposed "Features from Accelerated Segment Test (FAST)"\cite{rosten_machine_2006} which combines the machine learning approach to speed up the comparison. This method has seen significant performance enhancement for real-time computer vision applications. Also, this method has proven in several applications to be reliable due to high repeatability (see \cite{rosten_faster_2010}). Some example applications which use FAST are Klein's PTAM\cite{klein_parallel_2007} and Taylor's robust feature matching in 2.3 \si{\micro\second} \cite{taylor_robust_2009}. From this idea, Mair et al., proposed "Adaptive and Generic Corner Detection Based on the Accelerated Segment Test (AGAST)" corner detector\cite{mair_adaptive_2010}. This method generalized FAST corner detector to improve performance even in the generalized environment.

\subsubsection{Silent Region Detectors}
Instead of trying to detect corners as keypoints in the images, one may use local extrema of the responses of certain filters as keypoints. In particular, many approaches aim at approximating the Laplacian of a Gaussian(LoG), which, given an appropriate normalization, was shown to be scale invariant if applied at multiple image scales\cite{lindeberg_scale-space_1994}. Lowe\cite{lowe_distinctive_2004} obtains scale invariance by convolving the image with a Difference of Gaussians(DoG) kernel at multiple scales, retaining locations which are optima in scale as well as space. DoG is used since it is good approximation for the LoG and much faster to compute. An approximation to DoG has been proposed which, provided that scales are $\sqrt{2}$ apart, speeds up computation by a factor of about two, compared to the striaghtforward implementation of Gaussian convolution\cite{crowley_fast_2003}. Scale-space techniques have also been combined with the Harris approach in \cite{mikolajczyk_indexing_2001} which computes Harris corners at multiple scales and retains only those which are also optima of the LoG response across scales. Hessian detector\cite{bay_speeded-up_2008} is also proposed which is based on efficient-to-compute approximations to the Hessian matrix at different scales.

In recent years, scale invariance has been extended to consider local features which are invariant to affine changes \cite{mikolajczyk_affine_2002,brown_invariant_2002,schaffalitzky_multi-view_2002}. Affine-invariant detectors provide higher repeatability for large affine distortions\cite{lowe_distinctive_2004,mikolajczyk_affine_2002}, but are typically expensive to compute\cite{mikolajczyk_comparison_2005,moreels_evaluation_2007}.


% ===================================================================================================
\subsection{Keypoint Descriptors}
The next procedure for keypoint-based matching is constructing a keypoint descriptor for the local patch with regard to the detected keypoint. The aim of this process is to capture the most important and distinctive information content enclosed in the detected keypoints, such that the same structure can be recognized if encountered. To accomplish the distinctiveness in real time, the inherent difficulty lies in balancing two competing goals: high-quality description and low computation requirements. Considered in these aspects, the description algorithms are classified in real-value based\cite{lowe_distinctive_2004,ke_pca-sift:_2004,bay_speeded-up_2008} and binary-value based\cite{calonder_brief:_2010,leutenegger_brisk:_2011,alahi_freak:_2012,rublee_orb:_2011}.

\subsubsection{Real-Value based Descriptors}
The algorithms in this category rely on feature vector of an image region, where each dimension is a floating-point type (or a discretization of a float excluding binary). These algorithms use local image characteristics (gradients, spatial frequencies, and etc). to describe the local image patch. The similarity between two descriptors can be calculated by using the $L^2$ norm, Mahalanobis distance, etc. These descriptors have proven to be effective, and tackle issues such as scale, rotation, viewpoint, or illumination variation. A variety of features derived from the local image intensities have been proposed to derive robust feature descriptors. Early ideas include derivatives for rotationally invariant features\cite{schmid_local_1997}, derivatives of Gaussians of different order\cite{freeman_design_1991}, filter banks derived from complex functions \cite{schaffalitzky_multi-view_2002}, phase information \cite{carneiro_multi-scale_2003}. Many of these have been evaluated by \cite{mikolajczyk_performance_2005}. 

Amongst the best quality features currently in the literature is the SIFT\cite{lowe_distinctive_2004}. A 128-dimensional vector is obtained from a grid of histograms of oriented gradient. Its high descriptive power and robustness to illumination and viewpoint changes has rated it as the reference keypoint descriptor for the past decade. However, the high dimensionality of this descriptor makes SIFT prohibitively slow. PCA-SIFT\cite{ke_pca-sift:_2004} reduced the description vector from 128 to 36 dimensions using principal component analysis(PCA). Even the matching time is reduced, the time to build the descriptor is increased. So it leads to a small gain in speed and a loss of distinctiveness. The gradient location and orientation histogram(GLOH) descriptor\cite{mikolajczyk_performance_2005} belongs to the family of SIFT-like methods and has been shown to be more distinctive but also more expensive to compute than SIFT.  The robustness against various viewpoints is improved in \cite{yu_fully_2009} by simulating multiple deformations to the descriptive patch.

One of the widely used keypoints at the moment is clearly SURF\cite{bay_speeded-up_2008}. It has similar matching performances as SIFT but is much faster. It also relies on local gradient histograms. The Haar-wavelet responses are efficiently computed with integral images leading to 64 or 128-dimensional vectors. However, the dimensionality of the feature vector is still too high for large-scale applications such as image retrieval or 3D reconstruction.


\subsubsection{Binary-Value based Descriptors}
Another category of keypoint description is binary descriptors. These descriptors have a compact binary representation and limited computational requirements, computing the descriptor directly from pixel-level comparisons. This makes them an attractive solution to many modern applications, especially for mobile platforms where both compute and memory resources are limited. Each bit in the descriptor is the result of one comparison, and the descriptor is built from a set of pairwise intensity comparisons. Because the descriptor is constructed by simple comparison operation, these algorithms provide simple and inexpensive complexity. Also, in this category, Hamming distance (bitwise XOR followed by a bit count) is used to compute a similarity, and it replaces the usual Euclidean distance. Therefore, matching complexity also will be decreased. 

The first approach of this category is "Binary Robust Independent Elementary Features (BRIEF)"\cite{calonder_brief:_2010}. It uses a sampling pattern consisting of 128, 256, or 512 comparisons (equating to 128, 256, or 512 bits), with sample points selected randomly from an isotropic Gaussian distribution centered at the feature location. The obtained descriptor is not invariant to scale and rotation changes unless coupled with detector providing it. Calonder et al. also highlighted in their work that usually orientation detection reduces the recognition rate and should therefore be avoided when it is not required by the target application. Rublee et al. proposed the "Oriented Fast and Rotated BRIEF (ORB)" descriptor\cite{rublee_orb:_2011} to overcome the lack of rotation invariance of BRIEF. The descriptor is not only invariant to rotation, but also robust to noise. The sampling pattern employed in ORB uses 256 pairwise intensity comparisons. In contrast to BRIEF, it is constructed via machine learning, maximizing the descriptorâ€™s variance and minimizing the correlation under various orientation changes. Leutenegger et al. proposed a binary descriptor invariant to scale and rotation so called "Binary Robust Invariant Scalable Keypoints (BRISK)"\cite{leutenegger_brisk:_2011}. To build the descriptor bit-stream, a limited number of points in a symmetric pattern is used. Each point contributes to many pairs. The pairs are divided in short-distance and long-distance subsets. The long-distance subset is used to estimate the direction of the keypoint while the short-distance subset is used to build binary descriptor after rotating the sampling pattern. Overall, BRISK requires more computation and more storage space than either BRIEF or ORB. Similarly, Alahi et al. proposed "Fast Retina Keypoint (FREAK)" descriptor \cite{alahi_freak:_2012}. It is inspired by the human visual system. A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. FREAK requires lower storage space and shows more robust than BRISK.


% ===================================================================================================
\subsection{Keypoint Matching}
The final process in keypoint-based matching is searching for the most similar matches (or candidates) to high-dimensional vectors, also referred to as nearest neighbor matching. In general, the number of keypoints from a single image may be ranged from hundred to thousand. Therefore, in this process, huge number of comparison is executed. It becomes a bottleneck to entire process. To improve this problem, some literatures have been reported to construct an efficient matching data structure. There are two main aspects for robust and fast matching while maintaining the quality of matching - feature dimensionality reduction and efficient matching structure. The former is concerned with keypoint description. As mentioned above, binary descriptors offers fast matching speed because they can be computed by executing the XOR operation followed by a few bitwise instructions that can be performed quickly, especially on modern central processing units(CPUs). Usually millions of binary codes are compared only in less than a second\cite{ma_fast_2014}. Even though the distance between binary codes can be computed efficiently, using linear nearest neighbor search for exact matching is practical only for small datasets. For large-scale datasets, exact nearest neighbor search will lose its time performance. To solve this problem, various approximated nearest neighbor(ANN) search algorithms were proposed. These ANN methods are classified in two categories: partitioning trees and hashing techniques\cite{muja_scalable_2014}.


\subsubsection{Partitioning Trees}
The k-d tree\cite{bentley_multidimensional_1975,friedman_algorithm_1977} is one of the best known nearest neighbor algorithms. Arya et al.\cite{arya_optimal_1998} proposed an error bound approximate search method based on kd-tree. The algorithm used a priority queue to speed up the search. Meanwhile, Beis and Lowe\cite{beis_shape_1997} proposed a time bound approximate search. In practice the time-constrained approximation criterion has been found to give better results than the error-constrained approximate search. Various extensions of k-d tree algorithm were proposed\cite{silpa-anan_optimised_2008,sproull_refinements_1991,dasgupta_random_2008,jia_optimizing_2010}. In \cite{muja_fast_2009}, several approximated nearest neighbor algorithms were compared, and the multiple randomized k-d tree was the most effective.

Another approach of partitioning trees decomposes the space using various clustering algorithms instead of using hyper planes as in the case of the k-d tree and its variants. Example of suck decompositions include the hierarchical k-means tree \cite{fukunaga_branch_1975}, the GNAT \cite{brin_near_1995}, the anchors hierarchy \cite{moore_anchors_2000}, and the spill-tree \cite{liu_investigation_2004}. The vocabulary tree\cite{nister_scalable_2006} is searched by accessing a single leaf of a hierarchical k-means tree. Schindler et al. \cite{schindler_city-scale_2007} proposed a new way of searching the hierarchical k-means tree.

Moreover, another class of partitioning trees combined keypoint description method to construct matching structure\cite{lepetit_keypoint_2006,ozuysal_fast_2007,ozuysal_fast_2010}. Lepetit and Fua\cite{lepetit_keypoint_2006} formulated keypoint matching as a classification problem using Randomized Trees as classifiers. \"{O}zuysal et al.\cite{ozuysal_fast_2007} simplified this approach structurally by adopting a na\"{i}ve Bayes approach, thus simplifying the trees to "ferns." Taylor et al.\cite{taylor_robust_2009} presented another training-based keypoint recognition approach, which, during the training step, builds coarsely quantized histogram based representations. These approaches trained with huge number of training images which are synthesized by various transformation. So, these approaches take a long time to train a offline database, but these are able to improve online matching quality.


\subsubsection{Hashing}

In contrast to tree approaches, hashing is usually used for binary features. \cite{salakhutdinov_semantic_2009} proposed the notion of semantic hashing when they learn a deep graphical model that maps documents to small binary codes. When the mapping is performed such that close features are mapped to close codes (in Hamming space), the nearest neighbor matching can be efficiently performed by searching for codes that differ by a few bits from the query code. A similar approach is used by Torralba et al.\cite{torralba_small_2008} who learn compact binary codes from images with the goal of performing real-time image recognition on a large dataset of images using limited memory. Weiss et al.\cite{weiss_spectral_2009} formalize the requirements for good codes and introduce a new technique for efficiently computing binary codes. 

Performing ANN search by examining all the points in a Hamming radius works efficiently when the distance between the matching codes is small. When this distance gets larger the number of points in the Hamming radius gets exponentially larger, making the method unpractical. This is the case for binary value features. where the minimum distance between matching features can be larger than 20 bits. In cases, the best known hashing based nearest neighbor technique is locality sensitive hashing(LSH)\cite{gionis_similarity_1999, andoni_near-optimal_2006}, which uses a large number of hash functions with the property that the hashes of elements that are close to each other are also likely to be close. Variants of LSH such as multi-probe LSH\cite{lv_multi-probe_2007} improves the storage costs by reducing the number of hash tables, and LSH Forest \cite{bawa_lsh_2005} adapts better to the data without requiring hand tuning of parameters.

The different hashing algorithms provide theoretical guarantees on the search performance and have been successfully used in a number of projects, however, \cite{muja_scalable_2014} shows that in practice they are usually outperformed by algorithms using space partitioning structures such as the randomized k-d trees and similar approaches.

These approaches similarly, to provide efficient searching the approximated nearest neighbor, in the offline training phase, they executed large amount additional process to construct efficient searching structure. Since it is better to take more process in offline phase and provide more efficient searching result, so these techniques are widely executed. This idea is similar with our approach, our proposed algorithm performs large amount of offline process to filter out un-distinguishable keypoints, and provides more efficient and robust matching quality. 



%======================================================================================================
\subsection{Keypoint Filtering}
The previous processes described how to extract repeatable keypoint, how to describe a local texture, and how to construct an efficient searching structures. However, they do not consider which keypoints to be stored for providing high quality of matches. To cover this issue, several literatures are proposed. 

\subsubsection{Filtering in keypoints detection}
The process to decide which keypoints to be stored can be performed in keypoints detection. The usual keypoints detection algorithms calculates corner response function(CRF) which represents how much the points is repeatably extracted(\textit{Repeatability}). There are two filtering scheme using the CRF: thresholding and non-maximum suppression. In the early stage of keypoint detection, thresholding scheme was widely used. Harris and Stephan\cite{harris_combined_1988} defined the Harris score from the second-order moment image gradient matrix. Similarly, Shi and Tomasi\cite{shi_good_1994}, by mathematical analysis, led that it is better to use the smallest eigenvalue of image gradient matrix as the corner strength function. A number of suggestions\cite{noble_descriptions_1989,kenney_condition_2003} have been made of how to compute the corner strength from the matrix and filtered by thresholding. Also, while Harris score is scale variant, scale invariant keypoint detectors are also proposed. In \cite{foo_pruning_2007}, keypoints are filtered by setting a threshold for local peaks of a scale space in SIFT because a low peak coming from low contrast of image intensity is unstably computed. 
In the recent stage, non-maximum suppression(NMS) is used more popular, because this scheme can considers spatial relationship of keypoints in addition to keypoint response. Since keypoints with high response may appear in continuous pixels, pixels with only local maximum of the responses are selected and the others are suppressed. Some researches such as \cite{lowe_distinctive_2004,rublee_orb:_2011,mikolajczyk_indexing_2001} used NMS based on the Harris score. \cite{bay_speeded-up_2008} used the determinant of the Hessian matrix. So, each of keypoints detection algorithms defined their own corner response functions and based on those, they filters corner by non-maximum suppression. These non-maximum suppression has been extended to block based method\cite{neubeck_efficient_2006}, an adaptive method\cite{brown_multi-image_2005} and an efficient method with Suppression via Disk Covering\cite{gauglitz_efficiently_2011}.

\subsubsection{Filtering Considering Distinctiveness}
Even though keypoints are detected in different viewpoints with high repeatability, they are not considered distinctiveness. For example, as shown Fig. \ref{fig:example_of_bad_features}, corners in a checkerboard can be stably detected, but it is hard to correctly match or distinguish them in different viewpoint. Therefore, such points are not appropriate for robust keypoint matching and those should be filtered out. Therefore, stored keypoints must consider the distinctiveness to provide precise matching performance. However, it seems that slight literatures realized this problem. In \cite{knapek_selecting_2000,oerlemans_interest_2008}, a feature vector is computed from local texture and then compared with other feature vectors. By removing pixels that have similar feature vectors, only distinctive keypoints can be selected. In contrast those researches, we propose keypoint filtering criteria considering with not only repeatability, but also distinctiveness and each keypoint's variance. 

\subsection{Keypoint Matching Systems}
Combined with these keypoint matching algorithms, there are lots of keypoints matching applications proposed. In table \ref{tab:matching_applications}, these systems are listed along with the algorithms that are used in.  This compilation is not meant to be exhaustive, and the short bullet points do not do justice to specific features and contributions of the listed systems. Rather, it is meant to give an overview of the applications of visual tracking and the algorithms that have been employed for different components.


\begin{table*}[t]
  \caption{Keypointbased Image Matching Systems}
  \label{tab:matching_applications}
  \centering
  % \begin{tabular}{l l l l}
  % \begin{tabularx}{\textwidth}{llllll}
  \begin{tabularx}{\textwidth}{p{0.25\textwidth}    % Reference
                               p{0.1\textwidth}    % Detector
                               p{0.15\textwidth}     % Descriptor
                               p{0.15\textwidth}    % Matching
                               p{0.2\textwidth}}     % Filtering
  	\hline
      Reference & Detector & Descriptor & Matching & Filtering \\
    \hline
    Bleser and Stricker (2008)\cite{bleser_advanced_2008} 	& FAST 			  & patch, warped 	        &  &  \\
    Carrera et al. (2007)\cite{carrera_robust_2007}         & Harris      & SURF                    & Linear nearest neighbor & NMS (Determinant of Hessian) \\
    Chekhlov et al. (2007)\cite{chekhlov_robust_2007}       & Shi-Tomasi  & SIFT-like               &  &  \\
    Davison et al. (2007)\cite{davison_monoslam:_2007}      & Shi-Tomasi  & patch, warped           &  &  \\
    Eade and Drummond (2006)\cite{eade_scalable_2006}       & FAST        & patch, warped           &  &  \\
    Klein and Murray (2007)\cite{klein_parallel_2007}       & FAST        & patch, warped           &  &  \\
    Lee and H\"{o}llerer (2008)\cite{lee_hybrid_2008}       & DoG         & Optical flow \& SIFT    &  k-d tree & NMS (DoG) \\
    Lepetit and Fua (2006)\cite{lepetit_keypoint_2006}      &             & Randomized Trees        & Randomized Trees &   \\
    Muja and Lowe (2012)\cite{muja_fast_2012}               & DoG         & SIFT                    & FLANN   & \\
    Nist\'{e}r et al. (2004)\cite{nister_visual_2004}       & Harris      & patch                   &  &  \\
    \"{O}zuysal et al. (2007)\cite{ozuysal_fast_2007}       &             & Ferns                   & Ferns  & \\
    Park et al. (2008)\cite{park_multiple_2008}             & not specified & Ferns                   & Ferns  & not specified \\
    Se et al. (2002)\cite{se_mobile_2002}                   & DoG         & scale, orientation      &  &  \\
    Skrypnyk and Lowe (2004)\cite{skrypnyk_scene_2004}      & DoG         & SIFT                    & k-d tree  &  \\
    Taylor et al. (2009)\cite{taylor_robust_2009}           & FAST        & trained histograms      &  &  \\
    Wagner et al. (2009)\cite{wagner_multiple_2009}         & FAST        & patch \& reduced SIFT   & a forest of spill trees & NMS (DoG) \\
    Wagner et al. (2010)\cite{wagner_real-time_2010}        & FAST        & patch, warped           &  &  \\
    Wiliams et al. (2007)\cite{williams_real-time_2007}     & FAST        & Randomized lists        &  &  \\
    \hline
    \end{tabularx}
  % \end{tabular}
\end{table*}
